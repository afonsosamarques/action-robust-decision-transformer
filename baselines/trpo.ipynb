{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from gym_wrapper import GymWrapperRecorder\n",
    "\n",
    "from datasets import Dataset\n",
    "from sb3_contrib import TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'HalfCheetah-v4'\n",
    "env = GymWrapperRecorder(gym.make(env_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"trpo_{env_name.split('-')[0].lower()}\"\n",
    "model = TRPO(\"MlpPolicy\", env, verbose=0, device=\"cpu\")\n",
    "model.learn(total_timesteps=1000*1000)\n",
    "model.save(f\"./agents/{model_name}\")\n",
    "\n",
    "# model = PPO.load(f\"./agents/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.get_all_episodes()\n",
    "d = defaultdict(list)\n",
    "returns = []\n",
    "\n",
    "for t in td:\n",
    "    d['observations'].append([list(obs) for obs in t['observations']])\n",
    "    d['actions'].append([list(act) for act in t['actions']])\n",
    "    d['rewards'].append(list(t['rewards']))\n",
    "    d['dones'].append(list(t['dones']))\n",
    "    returns.append(sum(t['rewards']))\n",
    "\n",
    "ds = Dataset.from_dict(d)\n",
    "print(f\"Episode returns | Avg: {np.round(np.mean(returns), 4)} | Std: {np.round(np.std(returns), 4)} | Min: {np.round(np.min(returns), 4)} | Median: {np.round(np.median(returns), 4)} | Max: {np.round(np.max(returns), 4)}\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk(f'./datasets/{model_name}_train') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TRAJECTORIES = 1000\n",
    "MAX_EP_LEN = 1000\n",
    "RETURNS_SCALE = 1000.0\n",
    "\n",
    "eval_dict = defaultdict(list)\n",
    "obs = env.restart()\n",
    "for i in range(TOTAL_TRAJECTORIES):\n",
    "    ep_return = 0\n",
    "    ep_len = 0\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        ep_return += reward\n",
    "        ep_len += 1\n",
    "\n",
    "        if done or ep_len == MAX_EP_LEN - 1:\n",
    "            eval_dict['iter'].append(i)\n",
    "            eval_dict['ep_length'].append(ep_len)\n",
    "            eval_dict['ep_return'].append(ep_return)\n",
    "            break\n",
    "\n",
    "print(f\"Episode lengths | Avg: {np.round(np.mean(eval_dict['ep_length']), 4)} | Std: {np.round(np.std(eval_dict['ep_length']), 4)} | Min: {np.round(np.min(eval_dict['ep_length']), 4)} | Median: {np.round(np.median(eval_dict['ep_length']), 4)} | Max: {np.round(np.max(eval_dict['ep_length']), 4)}\")\n",
    "print(f\"Episode returns | Avg: {np.round(np.mean(eval_dict['ep_return']), 4)} | Std: {np.round(np.std(eval_dict['ep_return']), 4)} | Min: {np.round(np.min(eval_dict['ep_return']), 4)} | Median: {np.round(np.median(eval_dict['ep_return']), 4)} | Max: {np.round(np.max(eval_dict['ep_return']), 4)}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.get_all_episodes()\n",
    "d = defaultdict(list)\n",
    "for t in td:\n",
    "    d['observations'].append([list(obs) for obs in t['observations']])\n",
    "    d['actions'].append([list(act) for act in t['actions']])\n",
    "    d['rewards'].append(list(t['rewards']))\n",
    "    d['dones'].append(list(t['dones']))\n",
    "\n",
    "ds = Dataset.from_dict(d)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk(f'./datasets/{model_name}_eval')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp-adt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
