{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import wandb\n",
        "\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from huggingface_hub import login, list_models\n",
        "from transformers import DecisionTransformerConfig, Trainer, TrainingArguments\n",
        "\n",
        "from trainable_dt import DecisionTransformerGymDataCollator, TrainableDT\n",
        "\n",
        "#\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from access_tokens import HF_WRITE_TOKEN, WANDB_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "login(token=HF_WRITE_TOKEN)\n",
        "wandb.login(key=WANDB_TOKEN)\n",
        "os.environ[\"WANDB_PROJECT\"] = 'ARDT-Project'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "envs = {\n",
        "    0: \"walker2d-expert-v2\",\n",
        "    1: \"halfcheetah-expert-v2\",\n",
        "}\n",
        "\n",
        "chosen_env = envs[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and exploring the dataset: halfcheetah (expert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some notes:\n",
        "* This is a multi-dimensional, continuous environment. States are represented by 17 continuous dimensions; actions are represented by 7 continuous dimensions.\n",
        "* The state space includes the positions and velocities of multiple body parts of the robotic cheetah, which are continuous, unbounded, real-valued quantities.\n",
        "* The action space consists of torques applied to the joints, which are real-valued and thus continuous. They are however limited to the interval [-1, 1]. \n",
        "\n",
        "For more details: https://www.gymlibrary.dev/environments/mujoco/half_cheetah/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", chosen_env)['train']\n",
        "# dataset = load_from_disk(\"./rarl_halfcheetah_v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dataset elements: \", dataset[0].keys())\n",
        "print(\"Number of steps: \", len(dataset[0]['observations']))\n",
        "print(\"Size of state representation: \", len(dataset[0]['observations'][0]))\n",
        "\n",
        "dataset = dataset.rename_columns({'pr_actions': 'actions'})\n",
        "print(\"Size of action representation: \", len(dataset[0]['actions'][0]))\n",
        "# print(\"Size of action representation: \", len(dataset[0]['pr_actions'][0]))\n",
        "# print(\"Size of action representation: \", len(dataset[0]['adv_actions'][0]))\n",
        "\n",
        "print(\"Reward type: \", type(dataset[0]['rewards'][0]))\n",
        "print(\"Done flag: \", type(dataset[0]['dones'][0]))\n",
        "print(\"Rewards len: \", len(dataset[0]['rewards']))\n",
        "print(\"Dones len: \", len(dataset[0]['dones']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RETURNS_SCALE = 1000.0\n",
        "CONTEXT_SIZE = 20"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While most datasets on the hub are ready to use out of the box, sometimes we wish to perform some additional processing or modification of the dataset. \n",
        "\n",
        "In this case we wish to match the author's implementation (from the original paper), that is we need to:\n",
        "* Normalize each feature by subtracting the mean and dividing by the standard deviation.\n",
        "* Pre-compute discounted returns for each trajectory.\n",
        "* Scale the rewards and returns by a factor of 1000.\n",
        "* Augment the dataset sampling distribution so it takes into account the length of the expert agentâ€™s trajectories.\n",
        "\n",
        "In order to perform this dataset preprocessing, we will use a custom Data Collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see trainable_dt.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a trainable Decision Transformer (HF is not trainable by default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see trainable_dt.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# putting together the model we just built\n",
        "collator = DecisionTransformerGymDataCollator(dataset, context_size=CONTEXT_SIZE, returns_scale=RETURNS_SCALE)\n",
        "config = DecisionTransformerConfig(state_dim=collator.state_dim, \n",
        "                                   act_dim=collator.act_dim,\n",
        "                                   max_ep_len=collator.max_ep_len,\n",
        "                                   context_size=collator.context_size,\n",
        "                                   state_mean=list(collator.state_mean),\n",
        "                                   state_std=list(collator.state_std),\n",
        "                                   scale=collator.scale,)\n",
        "model = TrainableDT(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model_name is None:\n",
        "    raise Exception(\"Please provide a model name\")\n",
        "\n",
        "# we use the same hyperparameters are in the authors original implementation, but train for fewer iterations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=300,\n",
        "    per_device_train_batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.25,\n",
        "    use_mps_device=True,\n",
        "    report_to=\"wandb\",\n",
        "    push_to_hub=True,\n",
        "    run_name=model_name,\n",
        "    hub_model_id=model_name,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
