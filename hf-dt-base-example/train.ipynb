{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import interpreter_login, list_models\n",
        "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interpreter_login()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "envs = {\n",
        "    0: \"walker2d-expert-v2\",\n",
        "    1: \"halfcheetah-expert-v2\",\n",
        "}\n",
        "\n",
        "chosen_env = envs[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and exploring the dataset: halfcheetah (expert)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some notes:\n",
        "* This is a multi-dimensional, continuous environment. States are represented by 17 continuous dimensions; actions are represented by 7 continuous dimensions.\n",
        "* The state space includes the positions and velocities of multiple body parts of the robotic cheetah, which are continuous, unbounded, real-valued quantities.\n",
        "* The action space consists of torques applied to the joints, which are real-valued and thus continuous. They are however limited to the interval [-1, 1]. \n",
        "\n",
        "For more details: https://www.gymlibrary.dev/environments/mujoco/half_cheetah/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", chosen_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dataset elements: \", dataset['train'][0].keys())\n",
        "print(\"Number of steps: \", len(dataset['train'][0]['observations']))\n",
        "print(\"Size of state representation: \", len(dataset['train'][0]['observations'][0]))\n",
        "print(\"Size of action representation: \", len(dataset['train'][0]['actions'][0]))\n",
        "print(\"Reward type: \", type(dataset['train'][0]['rewards'][0]))\n",
        "print(\"Done flag: \", type(dataset['train'][0]['dones'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset['train'][0]['observations'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset['train'][0]['actions'][0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RETURNS_SCALE = 1000.0\n",
        "CONTEXT_SIZE = 20"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While most datasets on the hub are ready to use out of the box, sometimes we wish to perform some additional processing or modification of the dataset. \n",
        "\n",
        "In this case we wish to match the author's implementation (from the original paper), that is we need to:\n",
        "* Normalize each feature by subtracting the mean and dividing by the standard deviation.\n",
        "* Pre-compute discounted returns for each trajectory.\n",
        "* Scale the rewards and returns by a factor of 1000.\n",
        "* Augment the dataset sampling distribution so it takes into account the length of the expert agentâ€™s trajectories.\n",
        "\n",
        "In order to perform this dataset preprocessing, we will use a custom Data Collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DecisionTransformerGymDataCollator:\n",
        "    return_tensors: str = \"pt\"  # pytorch tensors\n",
        "    context_size: int = 1  # length of trajectories we use in training\n",
        "    state_dim: int = 1  # size of state space\n",
        "    act_dim: int = 1  # size of action space\n",
        "    max_ep_len: int = 9999999  # max episode length in the dataset\n",
        "    scale: float = 1  # normalisation of rewards/returns\n",
        "    state_mean: np.array = None  # to store state means\n",
        "    state_std: np.array = None  # to store state stds\n",
        "    p_sample: np.array = None  # a distribution weighing episodes by trajectory lengths\n",
        "    n_traj: int = 0  # to store the number of trajectories in the dataset\n",
        "\n",
        "    def __init__(self, dataset, context_size, returns_scale):\n",
        "        self.dataset = dataset\n",
        "        # get dataset-specific features\n",
        "        self.max_ep_len = max([len(traj[\"rewards\"]) for traj in dataset])\n",
        "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
        "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
        "        self.context_size = context_size\n",
        "        self.returns_scale = returns_scale\n",
        "        # collect some statistics about the dataset\n",
        "        states = []\n",
        "        traj_lens = []\n",
        "        for obs in dataset[\"observations\"]:\n",
        "            states.extend(obs)\n",
        "            traj_lens.append(len(obs))\n",
        "        traj_lens = np.array(traj_lens)\n",
        "        states = np.vstack(states)\n",
        "        # use stats to produce normalisation constants\n",
        "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-8\n",
        "        self.n_traj = traj_lens.shape[0]\n",
        "        self.p_sample = traj_lens / sum(traj_lens)\n",
        "\n",
        "    def __call__(self, features):\n",
        "\n",
        "        def _discount_cumsum(x, gamma):\n",
        "            # return-to-go calculation\n",
        "            discount_cumsum = np.zeros_like(x)\n",
        "            discount_cumsum[-1] = x[-1]\n",
        "            for t in reversed(range(x.shape[0] - 1)):\n",
        "                discount_cumsum[t] = x[t] + gamma * discount_cumsum[t+1]\n",
        "            return discount_cumsum\n",
        "\n",
        "        # FIXME this is a bit of a hack to be able to sample from a non-uniform distribution\n",
        "        # the idea is that we re-sample with replacement from the dataset rather than just taking the batch\n",
        "        # this also means we can sample according to the length of the trajectories\n",
        "        batch_inds = np.random.choice(\n",
        "            np.arange(self.n_traj),\n",
        "            size=len(features),\n",
        "            replace=True,\n",
        "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
        "        )\n",
        "\n",
        "        # a batch of dataset features\n",
        "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
        "        \n",
        "        for ind in batch_inds:\n",
        "            traj = self.dataset[int(ind)]\n",
        "            start = random.randint(0, len(traj[\"rewards\"]) - 1)  # FIXME we are again randomising which feels dumb\n",
        "\n",
        "            # get sequences from the dataset\n",
        "            s.append(np.array(traj[\"observations\"][start : start + self.context_size]).reshape(1, -1, self.state_dim))\n",
        "            a.append(np.array(traj[\"actions\"][start : start + self.context_size]).reshape(1, -1, self.act_dim))\n",
        "            r.append(np.array(traj[\"rewards\"][start : start + self.context_size]).reshape(1, -1, 1))\n",
        "            d.append(np.array(traj[\"dones\"][start : start + self.context_size]).reshape(1, -1))\n",
        "            timesteps.append(np.arange(start, start + s[-1].shape[1]).reshape(1, -1))\n",
        "            # FIXME feels hacky/dumb/unnecessary timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
        "            rtg.append(\n",
        "                _discount_cumsum(np.array(traj[\"rewards\"][start:]), gamma=1.0)[\n",
        "                    : s[-1].shape[1]  # TODO check the +1 removed here\n",
        "                ].reshape(1, -1, 1)\n",
        "            )\n",
        "\n",
        "            # FIXME hacky... can't see the purpose; could be tied to +1 removed above\n",
        "            # if rtg[-1].shape[1] < s[-1].shape[1]:\n",
        "            #     rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
        "\n",
        "            # normalising and padding\n",
        "            tlen = s[-1].shape[1]\n",
        "            padlen = self.context_size - tlen\n",
        "            \n",
        "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
        "            s[-1] = np.concatenate(\n",
        "                [np.zeros((1, padlen, self.state_dim)) * 1.0, s[-1]], \n",
        "                axis=1,\n",
        "            )\n",
        "            \n",
        "            a[-1] = np.concatenate(\n",
        "                [np.ones((1, padlen, self.act_dim)) * -10.0, a[-1]],\n",
        "                axis=1,\n",
        "            )\n",
        "\n",
        "            r[-1] = np.concatenate(\n",
        "                [np.zeros((1, padlen, 1)) * 1.0, r[-1]], \n",
        "                axis=1,\n",
        "            )\n",
        "\n",
        "            d[-1] = np.concatenate(\n",
        "                [np.ones((1, padlen)) * 2.0, d[-1]], \n",
        "                axis=1,\n",
        "            )\n",
        "\n",
        "            rtg[-1] /= self.scale\n",
        "            rtg[-1] = np.concatenate(\n",
        "                [np.zeros((1, padlen, 1)) * 1.0, rtg[-1]], \n",
        "                axis=1,\n",
        "            ) \n",
        "\n",
        "            timesteps[-1] = np.concatenate([np.zeros((1, padlen)), timesteps[-1]], axis=1)\n",
        "\n",
        "            # masking: disregard padded values\n",
        "            mask.append(np.concatenate([np.zeros((1, padlen)), np.ones((1, tlen))], axis=1))\n",
        "\n",
        "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
        "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
        "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
        "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
        "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
        "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
        "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
        "\n",
        "        return {\n",
        "            \"states\": s,\n",
        "            \"actions\": a,\n",
        "            \"rewards\": r,\n",
        "            \"returns_to_go\": rtg,\n",
        "            \"timesteps\": timesteps,\n",
        "            \"attention_mask\": mask,\n",
        "        }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a trainable Decision Transformer (HF is not trainable by default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainableDT(DecisionTransformerModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        output = super().forward(**kwargs)\n",
        "\n",
        "        # add the DT loss; applied only to non-padding values in action head\n",
        "        action_targets = kwargs[\"actions\"]\n",
        "        attention_mask = kwargs[\"attention_mask\"]\n",
        "        action_preds = output[1]\n",
        "        act_dim = action_preds.shape[2]\n",
        "        \n",
        "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "\n",
        "        return {\"loss\": torch.mean((action_preds - action_targets) ** 2)}\n",
        "\n",
        "    def original_forward(self, **kwargs):\n",
        "        return super().forward(**kwargs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# putting together the model we just built\n",
        "collator = DecisionTransformerGymDataCollator(dataset[\"train\"], context_size=CONTEXT_SIZE, returns_scale=RETURNS_SCALE)\n",
        "config = DecisionTransformerConfig(state_dim=collator.state_dim, \n",
        "                                   act_dim=collator.act_dim,\n",
        "                                   max_ep_len=collator.max_ep_len,\n",
        "                                   context_size=collator.context_size,\n",
        "                                   state_mean=list(collator.state_mean),\n",
        "                                   state_std=list(collator.state_std),\n",
        "                                   scale=collator.scale,)\n",
        "model = TrainableDT(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just naming stuff\n",
        "my_env_name = \"dt-\" + chosen_env.split(\"-\")[0]\n",
        "models = sorted([m.modelId.split(\"/\")[-1] for m in list_models(author=\"afonsosamarques\")])\n",
        "models = [m for m in models if my_env_name in m]\n",
        "if len(models) > 0:\n",
        "    latest_version = [m.split(\"-\")[-1][1:] for m in models][-1]\n",
        "    new_version = \"v\" + str(int(latest_version) + 1)\n",
        "else:\n",
        "    new_version = \"v0\"\n",
        "model_name = my_env_name + \"-\" + new_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we use the same hyperparameters are in the authors original implementation, but train for fewer iterations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=250,\n",
        "    per_device_train_batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.25,\n",
        "    use_mps_device=True,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=model_name,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
