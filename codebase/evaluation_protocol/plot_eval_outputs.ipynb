{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import DecisionTransformerConfig\n",
    "\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./eval-outputs-pipeline\"\n",
    "MODEL_TYPES = ['dt', 'ardt-simplest', 'ardt-vanilla', 'ardt-full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_paths = sorted([DIR + \"/\" + r for r in os.listdir(DIR) if os.path.isdir(os.path.join(DIR, r))])\n",
    "results_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_results = defaultdict(list)\n",
    "\n",
    "for i, path in enumerate(results_paths):\n",
    "    with open(path + \"/env-adv.json\", \"r\") as f:\n",
    "        model_returns = json.load(f)['ep_return']\n",
    "    model_name = path.split(\"/\")[-1][:-10]\n",
    "    model_config = DecisionTransformerConfig.from_pretrained(\"afonsosamarques/\" + path.split(\"/\")[-1], use_auth_token=True)\n",
    "    done = False\n",
    "    for type in MODEL_TYPES:\n",
    "        if model_name.startswith(type + \"-\"):\n",
    "            done = True\n",
    "            model_type = type\n",
    "            models_to_results['type'].append(model_type)\n",
    "    if not done: continue\n",
    "    models_to_results['name'].append(model_name)\n",
    "    models_to_results['number'].append(i)\n",
    "    models_to_results['return_mean'].append(int(np.mean(model_returns)))\n",
    "    models_to_results['return_std'].append(int(np.std(model_returns)))\n",
    "    models_to_results['lambda1'].append(model_config.lambda1)\n",
    "    models_to_results['lambda2'].append(model_config.lambda2)\n",
    "    dataset = model_name.split(\"-\")[-1]\n",
    "    models_to_results['dataset'].append(dataset)\n",
    "    model_id = f\"{model_type} | {dataset} | l1 = {model_config.lambda1} | l2 = {model_config.lambda2}\"\n",
    "    models_to_results['id'].append(model_id)\n",
    "\n",
    "def get_length(length, max_length, part, nparts=4):\n",
    "    multiple = 0.65 if (part % nparts) == 0 else (0.90 if (part % nparts) == 1 else 0)\n",
    "    return max_length if length == max_length else max_length + int((max_length - length) * multiple)\n",
    "\n",
    "parts = [s.split(\"|\") for s in models_to_results['id']]\n",
    "counts = [i for i in range(len(parts[0]))]\n",
    "max_lengths = [max(len(part[i]) for part in parts) for i in range(len(parts[0]))]\n",
    "aligned_strings = [\"|\".join(part.ljust(get_length(len(part), max_length, ct)) for part, max_length, ct in zip(parts[i], max_lengths, counts)) for i in range(len(parts))]\n",
    "models_to_results['id'] = aligned_strings\n",
    "models_to_results['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_dataset = False  # FIXME\n",
    "datasets = list(set([m for m in models_to_results['dataset']]))\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_dataset:\n",
    "    dataset_idx = -1  # FIXME\n",
    "    dataset = datasets[dataset_idx]\n",
    "    models_to_results = {k: [v for i, v in enumerate(models_to_results[k]) if models_to_results['dataset'][i] == dataset] for k in models_to_results.keys()}\n",
    "models_to_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(model_type, idx):\n",
    "    if model_type == 'dt':\n",
    "        return plt.cm.get_cmap('Blues', len(models_to_results['name']))(idx)\n",
    "    elif model_type == 'ardt-simplest':\n",
    "        return plt.cm.get_cmap('Oranges', len(models_to_results['name']))(idx)\n",
    "    elif model_type == 'ardt-vanilla':\n",
    "        return plt.cm.get_cmap('Reds', len(models_to_results['name']))(idx)\n",
    "    elif model_type == 'ardt-full':\n",
    "        return plt.cm.get_cmap('Purples', len(models_to_results['name']))(idx)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Model type {model_type} not recognized.\")\n",
    "\n",
    "\n",
    "def get_ecolor(dataset, idx):\n",
    "    dataset_idx = datasets.index(dataset)\n",
    "    colors = [\n",
    "        'magenta',\n",
    "        'sienna',\n",
    "        'olivedrab',\n",
    "        'grey',\n",
    "    ]\n",
    "    return colors[dataset_idx]\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9, len(models_to_results['return_mean'])):\n",
    "    plt.scatter(models_to_results['return_mean'][i], models_to_results['return_std'][i], s=100, color=get_color(models_to_results['type'][i], i), edgecolors=get_ecolor(models_to_results['dataset'][i], i), linewidths=3, label=models_to_results['id'][i])\n",
    "plt.xlabel(\"Return Mean\")\n",
    "plt.ylabel(\"Return Std\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., ncol=1)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp-adt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
